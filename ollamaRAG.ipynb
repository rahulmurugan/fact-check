{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, uuid, re, pickle, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('.').resolve()))\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import requests  # simple Ollama client\n",
    "from src.setup_path import setup_path\n",
    "setup_path()\n",
    "from src.rag.preprocess1 import extract_text, extract_tables, extract_images_and_ocr\n",
    "from src.rag.embedding   import build_corpus, create_embeddings, build_faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = Path(\"data/Clinical Files\")      # raw PDFs\n",
    "PRE_DIR    = Path(\"preprocessed\")       # output of preprocess\n",
    "VDB_DIR    = Path(\"vectordb\")\n",
    "CLAIMS_JSON = Path(\"data/Flublok_Claims.json\")       # marketing claims input\n",
    "OUTPUT_DIR  = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "BATCH_SIZE = 64\n",
    "EMB_MODEL  = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â Preâ€‘processing done\n"
     ]
    }
   ],
   "source": [
    "PRE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "for pdf_path in sorted(DATA_DIR.glob(\"*.pdf\")):\n",
    "    out_dir = PRE_DIR / pdf_path.stem\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    if not (out_dir / \"text_chunks.jsonl\").exists():\n",
    "        extract_text(pdf_path, out_dir)\n",
    "    if not (out_dir / \"tables.jsonl\").exists():\n",
    "        extract_tables(pdf_path, out_dir)\n",
    "    if not (out_dir / \"image_ocr.jsonl\").exists():\n",
    "        extract_images_and_ocr(pdf_path, out_dir)\n",
    "print(\"Â Preâ€‘processing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‚ï¸  Loaded 15,922 records from preprocessed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahul/Desktop/fact_check/fact-check/venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "embed âœ index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15922/15922 [01:03<00:00, 249.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…  Stored 15,922 vectors to vectordb/index.faiss\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, pickle, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "index_path = VDB_DIR / \"index.faiss\"\n",
    "\n",
    "if not index_path.exists():\n",
    "    corpus = build_corpus(PRE_DIR)\n",
    "\n",
    "    model = SentenceTransformer(EMB_MODEL, device=\"cpu\")   # swap to \"cuda\" if it fits\n",
    "    dim   = model.get_sentence_embedding_dimension()\n",
    "    index = faiss.IndexFlatIP(dim)                         # for cosine (embs normalised)\n",
    "    metadata = []\n",
    "\n",
    "    for rec in tqdm(corpus, desc=\"embed âœ index\"):\n",
    "        text = (\n",
    "            rec.get(\"text\")\n",
    "            or \" \".join([w for line in rec.get(\"data\", []) for w in line])\n",
    "            or rec.get(\"ocr_text\", \"\")\n",
    "        )\n",
    "\n",
    "        vec = model.encode(text, normalize_embeddings=True)\n",
    "        index.add(np.expand_dims(vec.astype(\"float32\"), 0))\n",
    "        metadata.append(rec)\n",
    "\n",
    "    VDB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    faiss.write_index(index, str(index_path))\n",
    "    with (VDB_DIR / \"docstore.pkl\").open(\"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(f\"âœ…  Stored {len(metadata):,} vectors to {index_path}\")\n",
    "else:\n",
    "    print(\"ğŸ” FAISS index already present â€“ skipping embedding step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(str(VDB_DIR / \"index.faiss\"))\n",
    "with (VDB_DIR / \"docstore.pkl\").open(\"rb\") as f:\n",
    "    docstore: List[Dict] = pickle.load(f)\n",
    "sbert = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "def retrieve(query: str, k: int = 5) -> List[Dict]:\n",
    "    q_emb = sbert.encode([query], normalize_embeddings=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [docstore[i] | {\"score\": float(D[0][j])} for j,i in enumerate(I[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_chat(prompt: str, temperature: float = 0.0) -> str:\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    r = requests.post(OLLAMA_URL, json=payload, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"response\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corag_answer(claim: str, max_steps: int = 6) -> List[Dict]:\n",
    "    qa_pairs = []\n",
    "    q = ollama_chat(f\"Given the claim:\\n\\\"{claim}\\\"\\nGenerate one focused question to help verify it.\")\n",
    "    for _ in range(max_steps):\n",
    "        hits = retrieve(q, k=5)\n",
    "        context = \"\\n\".join([f\"[{h['score']:.2f}] {h.get('text') or h.get('ocr_text') or h['data']}\" for h in hits])\n",
    "        a = ollama_chat(f\"Claim: {claim}\\nQuestion: {q}\\nEvidence snippets:\\n{context}\\nAnswer the question succinctly.\")\n",
    "        qa_pairs.append({\"q\": q, \"a\": a, \"evidence\": hits})\n",
    "        follow = ollama_chat(\n",
    "            f\"Claim: {claim}\\nCollected Q/A so far:\\n{json.dumps(qa_pairs, indent=2)}\\n\"\n",
    "            \"Is the evidence sufficient to decide the claim? Answer only yes or no.\"\n",
    "        )\n",
    "        if follow.lower().startswith(\"y\"):\n",
    "            break\n",
    "        q = ollama_chat(\n",
    "            f\"Claim: {claim}\\nGiven the evidence and answers so far:\\n{json.dumps(qa_pairs, indent=2)}\\n\"\n",
    "            \"Ask one followâ€‘up question that would best help verify the claim.\"\n",
    "        )\n",
    "    return qa_pairs\n",
    "\n",
    "def torag_answer(claim: str, max_steps: int = 6, branches: int = 3) -> List[Dict]:\n",
    "    best_pairs = []\n",
    "    qs = [ollama_chat(f\"Claim: \\\"{claim}\\\"\\nGenerate question #{i+1} to verify.\") for i in range(branches)]\n",
    "    for _ in range(max_steps):\n",
    "        branch_pairs = []\n",
    "        for q in qs:\n",
    "            hits = retrieve(q, k=5)\n",
    "            ctx = \"\\n\".join([f\"[{h['score']:.2f}] {h.get('text') or h.get('ocr_text') or h['data']}\" for h in hits])\n",
    "            a = ollama_chat(f\"Claim: {claim}\\nQuestion: {q}\\nEvidence:\\n{ctx}\\nAnswer succinctly.\")\n",
    "            branch_pairs.append({\"q\": q, \"a\": a, \"evidence\": hits})\n",
    "        # elimination\n",
    "        prompt = (\n",
    "            f\"Claim: {claim}\\nHere are {len(branch_pairs)} questionâ€‘answer pairs with evidence.\\n\"\n",
    "            f\"{json.dumps(branch_pairs, indent=2)}\\n\"\n",
    "            \"Select the single pair most helpful to verify the claim. Return its index (0â€‘based).\"\n",
    "        )\n",
    "        idx = int(re.findall(r'\\d+', ollama_chat(prompt))[0])\n",
    "        best = branch_pairs[idx]\n",
    "        best_pairs.append(best)\n",
    "        follow = ollama_chat(\n",
    "            f\"Claim: {claim}\\nBest pairs so far:\\n{json.dumps(best_pairs, indent=2)}\\n\"\n",
    "            \"Is the evidence now sufficient? Answer yes or no.\"\n",
    "        )\n",
    "        if follow.lower().startswith(\"y\"):\n",
    "            break\n",
    "        # new followâ€‘up questions\n",
    "        qs = [\n",
    "            ollama_chat(\n",
    "                f\"Claim: {claim}\\nCurrent best evidence:\\n{json.dumps(best_pairs, indent=2)}\\n\"\n",
    "                f\"Generate followâ€‘up question #{i+1}.\"\n",
    "            )\n",
    "            for i in range(branches)\n",
    "        ]\n",
    "    return best_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_claim(claim: str, qa_pairs: List[Dict]) -> Dict:\n",
    "    prompt = (\n",
    "        f\"Claim: {claim}\\nEvidence Q/A:\\n{json.dumps(qa_pairs, indent=2)}\\n\"\n",
    "        \"Based on the evidence, respond with JSON {{\\\"verdict\\\": \\\"supported|refuted|failed\\\", \"\n",
    "        \"\\\"explanation\\\": \\\"concise justification\\\"}}\"\n",
    "    )\n",
    "    return json.loads(ollama_chat(prompt, temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CoRAG:   0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "corag_answer() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# -------- CoRAG pass ----------------------------------------------------\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m tqdm(claims[\u001b[33m\"\u001b[39m\u001b[33mclaims\u001b[39m\u001b[33m\"\u001b[39m], desc=\u001b[33m\"\u001b[39m\u001b[33mCoRAG\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     qa  = \u001b[43mcorag_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclaim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m       \u001b[38;5;66;03m# â† keyword arg fixes TypeError\u001b[39;00m\n\u001b[32m     10\u001b[39m     res = decide_claim(c[\u001b[33m\"\u001b[39m\u001b[33mclaim\u001b[39m\u001b[33m\"\u001b[39m], qa)\n\u001b[32m     11\u001b[39m     write_jsonl(Path(\u001b[33m\"\u001b[39m\u001b[33mcorag_out.jsonl\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     12\u001b[39m                 {\u001b[33m\"\u001b[39m\u001b[33mclaim\u001b[39m\u001b[33m\"\u001b[39m: c[\u001b[33m\"\u001b[39m\u001b[33mclaim\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mqa\u001b[39m\u001b[33m\"\u001b[39m: qa, **res})\n",
      "\u001b[31mTypeError\u001b[39m: corag_answer() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "corag_model = load_corag_model()   # whatever your real loader is\n",
    "torag_model = load_torag_model()\n",
    "\n",
    "def write_jsonl(path: Path, rec: dict):\n",
    "    with path.open(\"a\", encoding=\"utfâ€‘8\") as f:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# CoRAG pass â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for c in tqdm(claims[\"claims\"], desc=\"CoRAG\"):\n",
    "    qa  = corag_answer(corag_model, c[\"claim\"])   # model first!\n",
    "    res = decide_claim(c[\"claim\"], qa)\n",
    "    write_jsonl(Path(\"corag_out.jsonl\"),\n",
    "                {\"claim\": c[\"claim\"], \"qa\": qa, **res})\n",
    "\n",
    "# ToRAG pass â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for c in tqdm(claims[\"claims\"], desc=\"ToRAG\"):\n",
    "    qa  = torag_answer(torag_model, c[\"claim\"])\n",
    "    res = decide_claim(c[\"claim\"], qa)\n",
    "    write_jsonl(Path(\"torag_out.jsonl\"),\n",
    "                {\"claim\": c[\"claim\"], \"qa\": qa, **res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
