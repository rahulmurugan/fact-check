{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, uuid, re, pickle, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('.').resolve()))\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import requests  # simple Ollama client\n",
    "from src.setup_path import setup_path\n",
    "setup_path()\n",
    "from src.rag.preprocess1 import extract_text, extract_tables, extract_images_and_ocr\n",
    "from src.rag.embedding   import build_corpus, create_embeddings, build_faiss_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR   = Path(\"data/Clinical Files\")      # raw PDFs\n",
    "PRE_DIR    = Path(\"preprocessed\")       # output of preprocess\n",
    "VDB_DIR    = Path(\"vectordb\")\n",
    "CLAIMS_JSON = Path(\"data/Flublok_Claims.json\")       # marketing claims input\n",
    "OUTPUT_DIR  = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "BATCH_SIZE = 64\n",
    "EMB_MODEL  = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬†Pre‚Äëprocessing done\n"
     ]
    }
   ],
   "source": [
    "PRE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "for pdf_path in sorted(DATA_DIR.glob(\"*.pdf\")):\n",
    "    out_dir = PRE_DIR / pdf_path.stem\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    if not (out_dir / \"text_chunks.jsonl\").exists():\n",
    "        extract_text(pdf_path, out_dir)\n",
    "    if not (out_dir / \"tables.jsonl\").exists():\n",
    "        extract_tables(pdf_path, out_dir)\n",
    "    if not (out_dir / \"image_ocr.jsonl\").exists():\n",
    "        extract_images_and_ocr(pdf_path, out_dir)\n",
    "print(\"¬†Pre‚Äëprocessing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è  Loaded 15,922 records from preprocessed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahul/Desktop/fact_check/fact-check/venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "embed ‚ûú index: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15922/15922 [01:03<00:00, 249.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  Stored 15,922 vectors to vectordb/index.faiss\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "index_path = VDB_DIR / \"index.faiss\"\n",
    "\n",
    "if not index_path.exists():\n",
    "    corpus = build_corpus(PRE_DIR)\n",
    "\n",
    "    model = SentenceTransformer(EMB_MODEL, device=\"cpu\")   # swap to \"cuda\" if it fits\n",
    "    dim   = model.get_sentence_embedding_dimension()\n",
    "    index = faiss.IndexFlatIP(dim)                         # for cosine (embs normalised)\n",
    "    metadata = []\n",
    "\n",
    "    for rec in tqdm(corpus, desc=\"embed ‚ûú index\"):\n",
    "        text = (\n",
    "            rec.get(\"text\")\n",
    "            or \" \".join([w for line in rec.get(\"data\", []) for w in line])\n",
    "            or rec.get(\"ocr_text\", \"\")\n",
    "        )\n",
    "\n",
    "        vec = model.encode(text, normalize_embeddings=True)\n",
    "        index.add(np.expand_dims(vec.astype(\"float32\"), 0))\n",
    "        metadata.append(rec)\n",
    "\n",
    "    VDB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    faiss.write_index(index, str(index_path))\n",
    "    with (VDB_DIR / \"docstore.pkl\").open(\"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    print(f\"‚úÖ  Stored {len(metadata):,} vectors to {index_path}\")\n",
    "else:\n",
    "    print(\"üîé FAISS index already present ‚Äì skipping embedding step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(str(VDB_DIR / \"index.faiss\"))\n",
    "with (VDB_DIR / \"docstore.pkl\").open(\"rb\") as f:\n",
    "    docstore: List[Dict] = pickle.load(f)\n",
    "sbert = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "def retrieve(query: str, k: int = 5) -> List[Dict]:\n",
    "    q_emb = sbert.encode([query], normalize_embeddings=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [docstore[i] | {\"score\": float(D[0][j])} for j,i in enumerate(I[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_chat(prompt: str, temperature: float = 0.0) -> str:\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    r = requests.post(OLLAMA_URL, json=payload, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"response\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def corag_answer(claim: str, max_steps: int = 6) -> List[Dict]:\n",
    "#     qa_pairs = []\n",
    "#     q = ollama_chat(f\"Given the claim:\\n\\\"{claim}\\\"\\nGenerate one focused question to help verify it.\")\n",
    "#     for _ in range(max_steps):\n",
    "#         hits = retrieve(q, k=5)\n",
    "#         context = \"\\n\".join([f\"[{h['score']:.2f}] {h.get('text') or h.get('ocr_text') or h['data']}\" for h in hits])\n",
    "#         a = ollama_chat(f\"Claim: {claim}\\nQuestion: {q}\\nEvidence snippets:\\n{context}\\nAnswer the question succinctly.\")\n",
    "#         qa_pairs.append({\"q\": q, \"a\": a, \"evidence\": hits})\n",
    "#         follow = ollama_chat(\n",
    "#             f\"Claim: {claim}\\nCollected Q/A so far:\\n{json.dumps(qa_pairs, indent=2)}\\n\"\n",
    "#             \"Is the evidence sufficient to decide the claim? Answer only yes or no.\"\n",
    "#         )\n",
    "#         if follow.lower().startswith(\"y\"):\n",
    "#             break\n",
    "#         q = ollama_chat(\n",
    "#             f\"Claim: {claim}\\nGiven the evidence and answers so far:\\n{json.dumps(qa_pairs, indent=2)}\\n\"\n",
    "#             \"Ask one follow‚Äëup question that would best help verify the claim.\"\n",
    "#         )\n",
    "#     return qa_pairs\n",
    "# def torag_answer(claim: str, max_steps: int = 6, branches: int = 3) -> List[Dict]:\n",
    "#     best_pairs = []\n",
    "#     qs = [ollama_chat(f\"Claim: \\\"{claim}\\\"\\nGenerate question #{i+1} to verify.\") for i in range(branches)]\n",
    "#     for _ in range(max_steps):\n",
    "#         branch_pairs = []\n",
    "#         for q in qs:\n",
    "#             hits = retrieve(q, k=5)\n",
    "#             ctx = \"\\n\".join([f\"[{h['score']:.2f}] {h.get('text') or h.get('ocr_text') or h['data']}\" for h in hits])\n",
    "#             a = ollama_chat(f\"Claim: {claim}\\nQuestion: {q}\\nEvidence:\\n{ctx}\\nAnswer succinctly.\")\n",
    "#             branch_pairs.append({\"q\": q, \"a\": a, \"evidence\": hits})\n",
    "#         # elimination\n",
    "#         prompt = (\n",
    "#             f\"Claim: {claim}\\nHere are {len(branch_pairs)} question‚Äëanswer pairs with evidence.\\n\"\n",
    "#             f\"{json.dumps(branch_pairs, indent=2)}\\n\"\n",
    "#             \"Select the single pair most helpful to verify the claim. Return its index (0‚Äëbased).\"\n",
    "#         )\n",
    "#         idx = int(re.findall(r'\\d+', ollama_chat(prompt))[0])\n",
    "#         best = branch_pairs[idx]\n",
    "#         best_pairs.append(best)\n",
    "#         follow = ollama_chat(\n",
    "#             f\"Claim: {claim}\\nBest pairs so far:\\n{json.dumps(best_pairs, indent=2)}\\n\"\n",
    "#             \"Is the evidence now sufficient? Answer yes or no.\"\n",
    "#         )\n",
    "#         if follow.lower().startswith(\"y\"):\n",
    "#             break\n",
    "#         # new follow‚Äëup questions\n",
    "#         qs = [\n",
    "#             ollama_chat(\n",
    "#                 f\"Claim: {claim}\\nCurrent best evidence:\\n{json.dumps(best_pairs, indent=2)}\\n\"\n",
    "#                 f\"Generate follow‚Äëup question #{i+1}.\"\n",
    "#             )\n",
    "#             for i in range(branches)\n",
    "#         ]\n",
    "#     return best_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decide_claim(claim: str, qa_pairs: List[Dict]) -> Dict:\n",
    "    prompt = (\n",
    "        f\"Claim: {claim}\\nEvidence Q/A:\\n{json.dumps(qa_pairs, indent=2)}\\n\"\n",
    "        \"Based on the evidence, respond with JSON {{\\\"verdict\\\": \\\"supported|refuted|failed\\\", \"\n",
    "        \"\\\"explanation\\\": \\\"concise justification\\\"}}\"\n",
    "    )\n",
    "    return json.loads(ollama_chat(prompt, temperature=0.1))\n",
    "\n",
    "claims = json.load(CLAIMS_JSON.open())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_matches_for_claim(claim: str, candidates: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    For a given claim and its candidate evidence, use Mistral (via ollama_chat) to determine\n",
    "    which clinical evidence snippets best support the claim. Each match output has keys:\n",
    "      - document_name: source document name,\n",
    "      - matching_text: the text snippet from the evidence,\n",
    "      - score: the similarity score from the retrieval.\n",
    "    Output is a JSON array.\n",
    "    \"\"\"\n",
    "    evidence_str = \"\\n\".join(\n",
    "        [f\"[{c['score']:.2f}] Document: {c.get('source_pdf', 'Unknown')} | Text: {c.get('text') or ' '.join(sum(c.get('data', []), [])) or c.get('ocr_text', '')}\"\n",
    "         for c in candidates]\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are provided with a marketing claim and several candidate clinical evidence snippets extracted from various documents.\n",
    "Your task is to select the snippets that best support the claim. A claim may be supported by multiple pieces of evidence.\n",
    "For each supporting snippet, output a JSON object with the following keys:\n",
    "- \"document_name\": the name of the clinical document,\n",
    "- \"matching_text\": the exact text of the evidence snippet,\n",
    "- \"score\": the similarity score (as provided).\n",
    "If no evidence is relevant, output an empty JSON array.\n",
    "\n",
    "Claim:\n",
    "\"{claim}\"\n",
    "\n",
    "Candidate Evidence:\n",
    "{evidence_str}\n",
    "\n",
    "Return only a valid JSON array.\n",
    "\"\"\"\n",
    "    response = ollama_chat(prompt, temperature=0.0)\n",
    "    try:\n",
    "        matches = json.loads(response)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from LLM response for claim:\")\n",
    "        print(claim)\n",
    "        print(\"Response was:\", response)\n",
    "        matches = []\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing claims: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [02:12<00:00, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to outputs/ollama_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with CLAIMS_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    marketing_claims = json.load(f)\n",
    "\n",
    "final_results = {\"claims\": []}\n",
    "\n",
    "for c in tqdm(marketing_claims[\"claims\"], desc=\"Processing claims\"):\n",
    "    claim_text = c[\"claim\"]\n",
    "    \n",
    "    # Retrieve candidates from your prebuilt FAISS index\n",
    "    candidates = retrieve(claim_text, k=5)  # Adjust k as needed\n",
    "    \n",
    "    # Use the LLM (Mistral) to infer the best matching evidence from the candidates\n",
    "    match_sources = infer_matches_for_claim(claim_text, candidates)\n",
    "    \n",
    "    final_results[\"claims\"].append({\n",
    "        \"claim\": claim_text,\n",
    "        \"match_sources\": match_sources\n",
    "    })\n",
    "\n",
    "# Save final results as JSON\n",
    "output_file = OUTPUT_DIR / \"ollama_results.json\"\n",
    "with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_matches_with_reason(claim: str, candidates: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Uses Mistral via ollama to infer support and reasoning per candidate match.\n",
    "    Returns list of dicts with fields: document_name, matching_text, score, supports, reason\n",
    "    \"\"\"\n",
    "    evidence_str = \"\\n\".join(\n",
    "        [f\"[{c['score']:.2f}] Document: {c.get('source_pdf', 'Unknown')} | Text: {c.get('text') or ' '.join(sum(c.get('data', []), [])) or c.get('ocr_text', '')}\"\n",
    "         for c in candidates]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are provided with a marketing claim and a list of candidate evidence snippets from clinical documents.\n",
    "\n",
    "Your task is to analyze whether each snippet supports the claim. For each, output a JSON object with:\n",
    "- \"document_name\": source document name,\n",
    "- \"matching_text\": the full evidence text,\n",
    "- \"score\": similarity score (float),\n",
    "- \"supports\": true or false (does it support the claim?),\n",
    "- \"reason\": 1‚Äì2 sentence explanation.\n",
    "\n",
    "Here is the claim:\n",
    "\"{claim}\"\n",
    "\n",
    "Candidate Evidence:\n",
    "{evidence_str}\n",
    "\n",
    "Return only a JSON array of decisions, one per candidate.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        response = ollama_chat(prompt, temperature=0.0)\n",
    "        return json.loads(response)\n",
    "    except Exception as e:\n",
    "        print(\" JSON parsing failed for claim:\", claim)\n",
    "        print(\"Raw response:\", response)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running LLM with reasoning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [09:18<00:00, 62.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to outputs/ollama2_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with CLAIMS_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    marketing_claims = json.load(f)\n",
    "\n",
    "enhanced_results = {\"claims\": []}\n",
    "\n",
    "for c in tqdm(marketing_claims[\"claims\"], desc=\"Running LLM with reasoning\"):\n",
    "    claim_text = c[\"claim\"]\n",
    "    candidates = retrieve(claim_text, k=7)  # optional: increase k\n",
    "    detailed_matches = infer_matches_with_reason(claim_text, candidates)\n",
    "\n",
    "    enhanced_results[\"claims\"].append({\n",
    "        \"claim\": claim_text,\n",
    "        \"match_sources\": detailed_matches\n",
    "    })\n",
    "\n",
    "# Save to outputs/ollama2_results.json\n",
    "with (OUTPUT_DIR / \"ollama2_results.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(enhanced_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved to outputs/ollama2_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
