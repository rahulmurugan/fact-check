# fact-check#  Fact Check Solution Guide

This project matches marketing claims from a flu vaccine product (Flublok) against supporting evidence from clinical literature. It explores three approaches to build a fact-checking system:

1. **Basic TF-IDF Matching**
2. **Semantic RAG (FAISS + DeepSeek-Qwen)**
3. **LLM-Inference via Ollama + FAISS Retrieval**

---
## ğŸ“ Project Directory Structure

```bash
solstice-fact-check/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Marketing File Page/              # Contains Flublok_Claims.json
â”‚
â”œâ”€â”€ outputs/                              # Output from Ollama-based inference
â”‚   â”œâ”€â”€ ollama_results.json
â”‚   â””â”€â”€ ollama2_results.json
â”‚
â”œâ”€â”€ results/                              # Output from basic and RAG approaches
â”‚   â”œâ”€â”€ basic_results.json
â”‚   â””â”€â”€ rag_results.json
â”‚
â”œâ”€â”€ preprocessed/                         # Extracted text, tables, and figures
â”‚
â”œâ”€â”€ vectordb/                             # FAISS index and serialized docstore
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ docstore.pkl
â”‚
â”œâ”€â”€ src/                                  # Source code modules
â”‚   â”œâ”€â”€ matcher.py                        # TF-IDF logic
â”‚   â”œâ”€â”€ preprocess.py                     # PDF processing
â”‚   â”œâ”€â”€ setup_path.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ chunking.py
â”‚       â”œâ”€â”€ embedding.py
â”‚       â”œâ”€â”€ generation.py
â”‚       â”œâ”€â”€ preprocess1.py
â”‚       â”œâ”€â”€ retrieval.py
â”‚
â”œâ”€â”€ main1.ipynb                           # Basic TF-IDF notebook
â”œâ”€â”€ main(RAG).ipynb                       # RAG pipeline using HuggingFace model
â”œâ”€â”€ ollamaRAG.ipynb                       # Ollama-based LLM pipeline
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”‚
â”œâ”€â”€ Basic_approach.pdf
â”œâ”€â”€ Exercise Description.pdf
â”œâ”€â”€ RAG_method.pdf
â””â”€â”€ Fact_check_ollama.pdf
```

## âš™ï¸ 1. Basic Approach (TF-IDF + Cosine Similarity)

This method builds a simple document retrieval pipeline using:

- **TF-IDF vectorization** of clinical texts and claims.
- **Cosine similarity** to rank matches.

Each claim is compared against all extracted clinical paragraphs. The top-3 matches are returned with similarity scores.

### âœ… Highlights

- Fast and interpretable
- Good baseline for comparison

### âš ï¸ Limitations

- No semantic understanding of context or paraphrasing
- Lower quality matches (typically score 0.1â€“0.3)

ğŸ“„ Output: `results/basic_results.json`

---

## ğŸ§  2. RAG Pipeline (FAISS + DeepSeek-Qwen LLM)

This approach combines semantic retrieval with local LLM-based generation.

### ğŸ”§ Components

- **Retrieval**: Text is embedded using `MiniLM-L3-v2` and stored in a FAISS index.
- **LLM Inference**: Retrieved candidates are passed to:
  
  ```python
  model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
  llm_pipeline = pipeline("text-generation", model=model_id, max_length=256, device=-1)

This HuggingFace pipeline runs fully locally and selects supporting snippets per claim.

### âœ… Highlights
- Open-source and local-first (no API keys required)
- Lightweight and CPU-friendly
- Clean JSON output with matched source texts

### âš ï¸ Limitations
- Does not output `supports: true/false` or reasoning
- Relies heavily on good FAISS retrieval + prompt context

ğŸ“„ **Output**: `results/rag_results.json`

---

## ğŸ¤– 3. Ollama + FAISS Retrieval (LLM-Inference)

This is the most advanced method, using:

- **SBERT + FAISS** for top-k semantic retrieval
- **Mistral LLM via Ollama** for deeper reasoning and claim evaluation

### ğŸ”¹ Version 1: `ollama_results.json`
- Returns only snippets the model considers as supporting the claim
- Efficient but lacks justification

### ğŸ”¸ Version 2: `ollama2_results.json`
Each snippet includes:
- `supports`: true/false
- `reason`: short explanation of decision

### âœ… Highlights
- Most explainable method
- Good for audits and regulatory submission
- Avoids fluency bias by using strict LLM logic

### âš ï¸ Limitations
- Slower (multiple LLM calls per claim)
- High RAM usage (Mistral is heavy on CPU)

ğŸ“„ **Output**: `outputs/ollama_results.json`, `outputs/ollama2_results.json`

---

## ğŸ“Š Method Comparison

| Method           | Semantic | LLM Used                          | Reasoning | Score Quality     | Output File             |
|------------------|----------|-----------------------------------|-----------|--------------------|--------------------------|
| TF-IDF           | âŒ        | âŒ                                 | âŒ         | Low (0.1â€“0.3)       | `basic_results.json`     |
| RAG (DeepSeek)   | âœ…        | DeepSeek-R1-Distill-Qwen (1.5B)   | âŒ         | Medium (0.4â€“0.7)    | `rag_results.json`       |
| Ollama v1        | âœ…        | Mistral via Ollama                | âŒ         | High               | `ollama_results.json`    |
| Ollama v2        | âœ…        | Mistral via Ollama                | âœ…         | High               | `ollama2_results.json`   |

---

## ğŸš§ CoRAG and ToRAG (Future Work)

We are exploring multi-turn reasoning strategies to go beyond flat retrieval:

- **CoRAG**: Iterative QA chain that continues until confident
- **ToRAG**: Tree of Thought prompting with branch ranking and follow-up generation

### âš ï¸ These methods require:
- Many LLM calls per claim
- Deep reasoning trees and dynamic branching

---

## ğŸš« Current Limitation

Due to the memory load of multi-step Mistral inference, local execution causes:
- RAM exhaustion
- Kernel crashes
- Slow iteration and batching issues

---

## â˜ï¸ Planned Deployment

We plan to move CoRAG/ToRAG to the cloud using:
- Google Colab (GPU)
- Hugging Face Inference Endpoints
- Replicate or Modal

This will support:
- Scalable, distributed LLM inference
- Deterministic, traceable outputs
- Production-ready experimentation

---

## ğŸš€ How to Run

```bash
pip install -r requirements.txt
